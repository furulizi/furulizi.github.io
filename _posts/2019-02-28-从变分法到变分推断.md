---
layout: post
title: "分类线性模型"
date: 2018-06-10 17:07:20 +0300
description: PRML 第四章 分类线性模型 # Add post description (optional)
img:  # Add image post (optional)
tags: [PRML]
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

### 从变分法到变分推断

> 在概率模型的应用中，核心的任务是计算给定观测数据的条件下，计算隐含变量的后验概率分布以及期望。对于隐含变量个数多且形式复杂的情况，直接求解后验概率分布几乎不可能，退而求近似解是个不错的解决思路。近似方法也分为两大类：马尔科夫蒙特卡洛采样的随机近似以及变分推断。本文将要讲述的就是变分推断。由于我们描述变分推断时会涉及变分法， 故我们从变分法说起。

#### 变分法的起源

&emsp;&emsp;变分法是求解泛函极值的一个基础方法，“泛函”一词作为表述也是源自于变分法。变分法实际上指的是古典变分法，古典变分法研究的是泛函极值问题，现代变分法属于泛函分析的内容，处理的更多是泛函的临界点，研究泛函水平流形的拓扑性质。既然是求解泛函的极值，那么我们就先从一个高中物理问题开始，理解“泛函”的定义。

&emsp;&emsp;1696年6月，瑞士数学家约翰·伯努利提出了一个问题，如果在空间上有两个点，这两个点之间用无摩擦力点轨道连接，此时有一个小球从点A滚到点B，那么怎么设计这个轨道可以使得小球从A到B的时间最短。如图，从A点到B点我们可以画出各种各样的轨道曲线，包括红色的直线。像这种问题，不是在一个函数中求极值，而是在一个函数组中求最优的函数曲线，将函数作为“自变量”输入，将一个实数输出，这就是“泛函”。虽然我们知道最速下降曲线就是这个问题的解，但是我们还是从1696年的视角来重现变分法基本原理的推导过程吧。

<div align='center'>
	<img src='https://raw.githubusercontent.com/furulizi/furulizi.github.io/master/assets/img/variance-fig1.png' />
</div>
&emsp;&emsp;如图，我们加上坐标轴（假如考虑纵坐标轴向下为正方向），首先，我们有动能定理。
$$
\begin{align}
\because & mgy=\frac{1}{2}mv^2 \\
\therefore & v=\sqrt{2gy} \\
\end{align}
$$

&emsp;&emsp;然后，瞬时速度可以表达成微分的形式。
$$
\begin{align}
\because & v=\frac{ds}{dt} \\
\therefore & dt=\frac{ds}v \\
\end{align}
$$
&emsp;&emsp;以及，微分的定义。
$$
\begin{align}
ds = \sqrt{(dx)^2 + (dy)^2}=dx \sqrt{1+(\frac{dy}{dx})^2}
\end{align}
$$
&emsp;&emsp;所以，消耗的时间为
$$
\begin{align}
T = \int dt&=\int (\frac{ds}{v})= \int \frac{\sqrt{dx^2+dy^2}}{\sqrt{2gy}} = \int \frac{\sqrt{1+y^{'}}}{\sqrt{2gy}}dx
\end{align}
$$

> 相信到这一步，高中物理和高等数学学习扎实的同学都能理解透彻了。但是细心的同学也会问到，为什么最后写成$x$的积分，而不是$y$的积分呢？因为$y(x)$是$x$的函数，而$T$是函数$y(x)$的函数，对$x$积分，且积分限一确定（小球的初末位置确定），$T[y]$就跟$x$无关。

&emsp;&emsp;$T[y]$就是所谓的泛函。泛函指的是一种定义域为函数，值域为实数的函数。注意，泛函不是复合函数，复合函数的定义域并非函数。

#### 泛函极值与欧拉-拉格朗日方程

&emsp;&emsp;我们继续讨论上述例子，我们希望找到一条曲线，使得消耗的时间最短。假设泛函数$T[y]$存在一个$y(x)$，使得泛函数$T[y]$取得极值，那么对于任意函数$\eta(x)$，给定任意一个$\epsilon \geq 0$， 函数组可表示为：$\widetilde y(x)=y(x)+\epsilon \eta (x)$。由于函数$y(x)$的值的积分边界上是固定值（A，B两点固定），$\eta(x_1) = \eta(x_2) = 0$，这也就保证了$\widetilde y(x)$正好通过两个固定的边界点。这种表示方法的好处在于$\epsilon$控制的变化量很微小，将函数组限制在极值附近，另外$\eta (x)$表示任意函数，意味着函数组可以变化的方式为任意的。还需注意到一点，当$\epsilon \to 0$时，$\widetilde y(x) \to y(x)$，也就是泛函数的极值找到了。如下图所示，红色直线为使得泛函数$T[y]$取得极值的函数，蓝色曲线表示随机扰动$\eta (x)$，绿色曲线表示$\epsilon=1$的情况。

<div align='center'>
        <img src='https://raw.githubusercontent.com/furulizi/furulizi.github.io/master/assets/img/variance-fig2.png' />
</div>
&emsp;&emsp;为了描述方便且跟[PRML](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)书本一致，我们重新定义一下数学表示符号。泛函的形式为
$$
F[y]=\int G(y(x),y^{'}(x),x)dx
$$
欧拉-拉格朗日方程为
$$
\frac{\partial G}{\partial y}-\frac{d}{dx}(\frac{\partial G}{\partial y^{'}}) =0
$$

欧拉-拉格朗日方程可以求解泛函下的极值，这里$G$是已知的。它的思想源自于微积分中“可导的极值点一定是临界点”，自然地，当我们假设泛函的解已知，那么解必然使得泛函取得极值，只要在泛函中加入任意扰动$\eta (x)$ ，用一个很小的值$\epsilon$来限制它，使$\epsilon$趋向于0，则扰动趋向于0，则泛函收敛至极值。这个思路巧妙在将函数的求导问题转换成一个单变量求导问题。所以，在最速下降曲线问题上，$G(y,y^{'},x) = \frac {\sqrt{1+y^{'}}}{\sqrt {2gy}}$形式已知，写出$\frac {dG(y,y^{'},x)}{dt}$，利用微积分的知识就可以将最速下降曲线写成参数方程的形式：
$$
\begin{equation}
\left\{
             \begin{array}{lr}
             t=r(\theta-sin \theta)\\
             y=r(1-cos \theta)\\
             \end{array}
\right.
\end{equation}
$$
还有一个公理可以使用欧拉-拉格朗日方程证明：两点之间直线距离最短，利用$I=\int_{x_1}^{x_2}G(y,y^{'},x)dx = \int_{x_1}^{x_2} \sqrt {1+y^{'2}} dx$，求该泛函的极值，最后可得$y^{'}=a$，其中$a$是一个常量。

#### 变分推断

&emsp;&emsp;变分法是为了求泛函的最优化问题，现在我们将变分最优化的思想应用到推断问题上，便是变分推断。假设我们有一个贝叶斯模型，其中每个参数都有一个先验概率分布，这个模型也可以有隐含变量以及参数，我们把所有隐含变量和参数组成的集合，记做$Z$。同样地，把所有可观测变量的集合记作$X$，那么我们搜集到的$N$个独立同分布的样本对应的可观测变量和隐含变量分别是$X={x_1,x_2,...,x_N}$，$Z={z_1,z_2,...,z_N}$。概率模型只是确定了联合概率分布$P(X,Z)$，我们的目标应该是找到后验概率$P(Z|X)$，以及计算关于该后验概率分布的期望等性质。

&emsp;&emsp;首先写出对数边缘概率的分解式子，
$$
lnP(X) = L(q) + KL(q||p)
$$
其中，
$$
\begin{align}
L(q) = \int q(Z)\ln \frac {p(X,Z)}{q(Z)}dZ \\
KL(q||p) = - \int q(Z)\ln \frac {p(Z|X)}{q(Z)}dZ
\end{align}
$$
&emsp;&emsp;上述的分解式子的原理跟EM算法的一致，在文末的附录我也做了严谨的推导。现在我们先感性地理解一下他们的意义。首先，$lnP(X)$是对概率模型中可观测变量的概率分布做了对数变换，它表示概率模型中出现这$N$个样本的概率的对数。那么分解成$L(q)$和$KL(q||p)$的作用在于引进一个新的概率分布$q(Z)$来近似$p(Z|X)$，KL散度正是度量$q(Z)$与$p(Z|X)$近似的情况。根据KL散度的定义，$q(Z)$与$p(Z|X)$越像，则积分内的$ln \frac {p(Z|X)}{q(Z)}$就越趋向于0，那么KL散度就越小。$L(q)$也被一些资料称为ELOW(Evidence Lower Bound)，而当我们优化$q(Z)$令KL散度最小时，也就是令$L(q)$最大，故而称$L(q)$为下界。另外，我们留意到$L(q)$和$KL(q||p)$的表达式很像，但$KL(q||p)$中有我们未知的$p(Z|X)$，自然地想法是**求下界$L(q)$的最大值来等价求$KL(q||p)$的最小值**。

&emsp;&emsp;讨论至此，我们应该清楚目标了。找到一个合适的$q(Z)$来近似$p(Z|X)$，使得下界$L(q)$达到最大。这跟我们前面讨论约翰·伯努利提出的变分法的例子有些类似。变分法的例子就是找到一条合适的曲线，使得小球从A点到B点花费的时间最短。这两个例子如果抛弃了物理或者概率的背景，都可以当成是一个泛函问题，最终求一个泛函的极值。但是由于学科的差异，古典变分法中，可以通过写出$G(y,y^{'},x)$的形式，也可以通过欧拉拉格朗日方程求解最终的曲线$y$。在概率模型中，我们发现描述$L(q)$的式子里面还有$p(X,Z)$这种与$q(Z)$没有直接关系的因子。但是不同学科间通常也有不少可以借鉴的思路。物理学中的一个近似框架叫做[平均场理论](https://en.wikipedia.org/wiki/Mean_field_theory)，大概的思想是将数量巨大的互相作用的多体问题转化成每一个粒子处在一种弱周期场中的单体问题，通俗地说就是将$q(Z)$进行分解。当然，**分解概率分布**也是有条件的。其一是尽量将$Z$分解成互不相交的组，这样就能把$Z$表示出来，也不产生多余的计算。其二是分解的因子尽可能是我们熟悉的或者我们可计算的，如果分解成最后我们也无法计算的因子，分解就没有意义了。

&emsp;&emsp;我们将$q(Z)$进行分解，
$$
q(Z) = \prod{i}^{M}q_{i}(Z_{i})
$$
并将分解概率分布代入原来的$L(q)$可得
$$
L(q) = \int q_j(Z_j) \ln \tilde{p}(X,Z_j)dZ_j - \int q_j(Z_j) \ln q_j(Z_j) dZ_j + Const
$$
其中，定义了一个新的概率分布$\tilde{p}(X,Z_j)$，
$$
\ln \tilde{p}(X,Z_j) = \mathbb{E}_{i \neq j}[\ln p(X,Z)]+Const \\
$$
而注意到期望是关于定义在所有$z_{i}$下的$q$概率分布的期望，
$$
\mathbb{E}_{i \neq j}[\ln p(X,Z)] = \int \int ...(M-1个积分) \int \ln p(X,Z) \prod _{i \neq j} q_i dZ_i
$$
这样，我们看到的$L(q)$已经成功地写成关于$M$个分布$q_i(Z_i)$的泛函（原来只写成了关于$q(Z)$的泛函），并且根据分解概率分布的条件一，$Z_{i}$应该各不相交，我们可以先控制其他分量固定，再优化某一分量，是的$L(q)$取得最大值。也许我们把$L(q)$重新写出来，我们更容易发现$L(q)$取得最大值时关于$q_j(Z_j)$的充分条件。
$$
L(q) = \int q_j(Z_j) \ln \frac {\tilde{p}(X,Z_j)}{q_j(Z_j)}dZ_j) + Const
=-KL(q_j(Z_j)|| \tilde{p}(X,Z_j)) + Const
$$
**原来，求$L(q)$最大值等价于求$q_j(Z_j)$和$\tilde{p}(X,Z_j)$的KL散度的最小值，当且仅当在$q_j(Z_j) = \tilde{p}(X,Z_j)$时获得**。于是，很容易能写出最优解$q_j^*(Z_j)$的表达式
$$
\ln q_j^*(Z_j) = \mathbb{E}_{i \neq j}[\ln p(X,Z)]+Const \\
$$
那么，变分推断的工作接近尾声了。我们观察上式，左边是一个第$j$个概率分布密度$q_j(Z_j)$，右边是用其余$M-1$个概率分布密度$q_{i}(Z_i)$的期望，以及一个归一化常数（使概率分布满足$\int_i q_i(Z_i) dZ_i =1$）。这个也就是我们用$q(Z)$做近似$p(Z|X)$时需要的迭代式，可以先给定一系列$q_i(Z_i),i=2,3,...,M$的初值，根据迭代式求出$q_1^{(1)}(Z_1)$，再将$q_1^{(1)}(Z_1),q_3^(Z_3),q_4^(Z_4),...q_M(Z_M)$代入迭代式，求出$q_2^{(1)}(Z_2)$。如此迭代下去，不断比较$L(q)$取得局部极大值，最终求出$q(Z)$。

#### 变分推断实战

变分自编码器

